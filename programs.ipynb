{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\rishi\\OneDrive\\Desktop\\New folder\\Gen AI\\lyrics-generator\\backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting txt to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. JSON file saved as './lyrics/output.json'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def text_to_json(folder_path):\n",
    "    data = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, file_name), \"r\") as file:\n",
    "                data[file_name] = file.read()\n",
    "    return data\n",
    "\n",
    "folder_path = \"./lyrics/\"\n",
    "json_data = text_to_json(folder_path)\n",
    "\n",
    "output_path = os.path.join(folder_path, \"output.json\")\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Conversion completed. JSON file saved as '{output_path}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing all the newlines i.e. \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newlines removed from the JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def remove_newlines_from_json(json_file_path):\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "    \n",
    "    # Remove newlines from the JSON data\n",
    "    json_data_no_newlines = {key: value.replace(\"\\n\", \"\") if isinstance(value, str) else value for key, value in json_data.items()}\n",
    "    \n",
    "    # Write the modified JSON data back to the file\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(json_data_no_newlines, file, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "json_file_path = \"./lyrics/output.json\"  # Replace this with the path to your JSON file\n",
    "remove_newlines_from_json(json_file_path)\n",
    "print(\"Newlines removed from the JSON file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing the json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dil Ibadat.txt: ['Dil', 'ibadat', 'kar', 'raha', 'hai', 'Dhadkanein', 'meri', 'sunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhunZindagi', 'ki', 'shaak', 'se', 'loon', 'kuch', 'haseen', 'pal', 'main', 'chunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhunDil', 'ibadat', 'kar', 'raha', 'hai', 'Dhadkanein', 'meri', 'sunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhunZindagi', 'ki', 'shaak', 'se', 'lu', 'kuch', 'haseen', 'pal', 'main', 'chunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhunJo', 'bhi', 'jitne', 'pal', 'jiyun', 'Unhe', 'tere', 'sang', 'jiyunJo', 'bhi', 'kal', 'ho', 'ab', 'mera', 'Usse', 'tere', 'sang', 'jiyunJo', 'bhi', 'saansein', 'main', 'bharun', 'Unhe', 'tere', 'sang', 'bharunChahe', 'jo', 'ho', 'raasta', 'Usse', 'tere', 'sang', 'chalunDil', 'ibadat', 'kar', 'raha', 'hai', 'Dhadkanein', 'meri', 'sunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhunMujhko', 'de', 'tu', 'mitt', 'jaane', 'ab', 'khud', 'se', 'dil', 'mil', 'jaaneKyun', 'hai', 'yeh', 'itna', 'faaslaLamhe', 'ye', 'phir', 'na', 'aane', 'Inko', 'tu', 'na', 'de', 'jaaneTu', 'mujhpe', 'khudko', 'de', 'lutaaTujhe', 'tujh', 'se', 'todh', 'lu', 'Kahin', 'khud', 'se', 'jod', 'luMere', 'jismo', 'jaandian', 'Teri', 'khusboo', 'odh', 'luJo', 'bhi', 'saansein', 'main', 'bharun', 'Unhe', 'tere', 'sang', 'bharunChahe', 'jo', 'ho', 'raasta', 'Usse', 'tere', 'sang', 'chalunDil', 'ibadat', 'kar', 'raha', 'hai', 'Dhadkanein', 'meri', 'sunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhunBahaon', 'mein', 'de', 'bas', 'jaane', 'Seenay', 'mein', 'de', 'chhup', 'jaaneTujh', 'bin', 'main', 'jaaun', 'to', 'kahanTujh', 'se', 'hai', 'mujhko', 'paane', 'Yaadon', 'ke', 'woh', 'nazraaneIk', 'jinpe', 'hak', 'ho', 'bas', 'meraTeri', 'yaadon', 'mein', 'rahun', 'Tere', 'khwabon', 'mein', 'jagunMujhe', 'doondhey', 'jab', 'koi', 'Teri', 'aankhon', 'mein', 'milunJo', 'bhi', 'saansein', 'main', 'bharun', 'Unhe', 'tere', 'sang', 'bharunChahe', 'jo', 'ho', 'raasta', 'Usse', 'tere', 'sang', 'chalunDil', 'ibadat', 'kar', 'raha', 'hai', 'Dhadkanein', 'meri', 'sunTujh', 'ko', 'main', 'kar', 'lu', 'haasil', 'lagi', 'hai', 'yehi', 'dhun']\n",
      "Hamari adhuri kahani.txt: ['Paas', 'aaye', '..', 'Dooriyaan', 'phir', 'bhi', 'kam', 'naa', 'huiEk', 'adhuri', 'si', 'hamari', 'kahani', 'rahiAasmaan', 'ko', 'zameen', ',', 'ye', 'zaroori', 'nahiJaa', 'mile', '..', 'jaa', 'mile', '..', 'Ishq', 'saccha', 'wahiJisko', 'milti', 'nahi', 'manzilein', '..', 'manzilein', '..', 'Rang', 'thhe', ',', 'noor', 'thaJab', 'kareeb', 'tu', 'thaEk', 'jannat', 'sa', 'tha', ',', 'yeh', 'jahaan', '..', 'Waqt', 'ki', 'ret', 'pe', 'kuch', 'mere', 'naam', 'saLikh', 'ke', 'chhod', 'gaya', 'tu', 'kahaan', '..', 'Hamari', 'adhuri', 'kahani', '..', 'Hamari', 'adhuri', 'kahani', '..', 'Hamari', 'adhuri', 'kahani', '..', 'Hamari', 'adhuri', 'kahani', '..', 'Khushbuon', 'se', 'teri', 'yunhi', 'takra', 'gayeChalte', 'chalte', 'dekho', 'na', 'hum', 'kahaan', 'aa', 'gayeJannatein', 'agar', 'yahinTu', 'dikhe', 'kyon', 'nahinChaand', 'suraj', 'sabhi', 'hai', 'yahaan', '..', 'Intezaar', 'tera', 'sadiyon', 'se', 'kar', 'rahaPyaasi', 'baithi', 'hai', 'kab', 'se', 'yahaan', '..', 'Humari', 'adhoori', 'kahaniHumari', 'adhoori', 'kahani', '..', 'Humari', 'adhoori', 'kahaniHumari', 'adhoori', 'kahani', '..', 'Pyaas', 'ka', 'yeh', 'safar', 'khatam', 'ho', 'jayegaKuch', 'adhura', 'sa', 'jo', 'tha', 'poora', 'ho', 'jayegaJhuk', 'gaya', 'aasmaanMil', 'gaye', 'do', 'jahaanHar', 'taraf', 'hai', 'milan', 'ka', 'samaa', '..', 'Doliya', 'hain', 'saji', ',', 'khushbuein', 'har', 'kahinPadhne', 'aaya', 'Khuda', 'khud', 'yahaan', '..', 'Hamari', 'adhuri', 'kahaniHamari', 'adhuri', 'kahani', '..', 'Hamari', 'adhuri', 'kahaniHamari', 'adhuri', 'kahani', '..']\n",
      "Tu hi haqeeqat.txt: ['Tu', 'hi', 'haqeeqat', 'khaab', 'tu', ',', 'dariya', 'tu', 'hi', 'pyaas', 'tuTu', 'hi', 'dil', 'ki', 'bekaraari', 'tu', 'sukun', 'tu', 'sukunJaau', 'main', 'abb', 'jab', 'jis', 'jagah', 'paau', 'main', 'tujhko', 'uss', 'jagahSaath', 'hoke', 'na', 'ho', 'tu', 'hai', 'rubaru', 'rubaruTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamgawah', 'meraTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamnawa', 'meraAa', 'tujhe', 'inn', 'baahon', 'mein', 'bhar', 'ke', 'aur', 'bhi', 'kar', 'loon', 'mainkareebTu', 'juda', 'ho', 'toh', 'lage', 'hai', 'aata', 'jaata', 'har', 'pal', 'ajeebIss', 'jahaan', 'mein', 'hai', 'aur', 'na', 'hoga', 'mujhsa', 'koi', 'bhi', 'khushnaseebTune', 'mujhko', 'dil', 'diya', 'hai', 'main', 'hoon', 'tere', 'sabse', 'kareebMain', 'hi', 'toh', 'tere', 'dil', 'mein', 'hoon', ',', 'main', 'hi', 'toh', 'saanson', 'mein', 'basuTere', 'dil', 'ki', 'dhadkano', 'mein', 'main', 'hi', 'hoon', ',', 'main', 'hi', 'hoonTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamgawah', 'meraTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamnawa', 'meraKab', 'bhala', 'abb', 'yeh', 'waqt', 'guzre', 'kuchh', 'pata', 'chalta', 'hi', 'nahiJabse', 'mujhko', 'tu', 'mila', 'hai', 'hosh', 'kuchh', 'bhi', 'apna', 'nahiUff', 'yeh', 'teri', 'palakein', 'ghani', 'si', ',', 'chhanv', 'inki', 'hai', 'dilnashiAbb', 'kise', 'darr', 'dhup', 'ka', 'hai', 'kyun', 'ki', 'hai', 'yeh', 'mujhpe', 'bichhiTere', 'bina', 'na', 'saans', 'loon', 'tere', 'bina', 'na', 'main', 'jiuTere', 'bina', 'na', 'ek', 'pal', 'bhi', 'reh', 'saku', ',', 'reh', 'sakuTu', 'hi', 'haqeeqat', 'khaab', 'tu', ',', 'dariya', 'tu', 'hi', 'pyaas', 'tuTu', 'hi', 'dil', 'ki', 'bekaraari', 'tu', 'sukun', 'tu', 'sukunTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamgawah', 'meraTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamnawa', 'meraTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamgawah', 'meraTu', 'hamsafar', 'tu', 'hamkadam', 'tu', 'hamnawa', 'mera']\n",
      "Tum mile.txt: ['khwaabon', 'bina', 'nigaahen', 'meri', 'jee', 'rahi', 'thi', 'koi', 'nahi', 'tha', 'ye', 'akeli', 'meri', 'thi', 'zindagi', 'khaamosh', 'tha', 'hothon', 'pe', 'baatein', 'nahi', 'thi', 'koi', 'nahi', 'tha', 'ye', 'akeli', 'meri', 'thi', 'zindagi', 'tum', 'mile', 'to', 'mil', 'gaya', 'ye', 'jahaan', 'tum', 'mile', 'to', 'har', 'pal', 'hai', 'naya', 'tum', 'mile', 'to', 'sab', 'se', 'hai', 'fasla', 'tum', 'mile', 'to', 'jadoo', 'chhaa', 'gaya', 'tum', 'mile', 'to', 'jeena', 'aa', 'gaya', 'tum', 'mile', 'to', 'mene', 'paya', 'hai', 'khuda', 'palkein', 'moondein', 'chaahat', 'meri', 'so', 'rahi', 'thi', 'khushboo', 'hawaon', 'mein', 'thi', 'maine', 'nahi', 'mehsoos', 'ki', 'jaane', 'kahan', 'bahaarein', 'meri', 'khil', 'rahi', 'thi', 'khushboo', 'hawaaon', 'mein', 'thi', 'maine', 'nahi', 'mehsoos', 'ki', 'tum', 'mile', 'to', 'mehki', 'barishen', 'tum', 'mile', 'to', 'jaagi', 'khwahishen', 'tum', 'mile', 'to', 'rango', 'ka', 'hai', 'silsila', 'tum', 'mile', 'to', 'jadoo', 'chhaa', 'gaya', 'tum', 'mile', 'to', 'jeena', 'aa', 'gaya', 'tum', 'mile', 'to', 'maine', 'paya', 'hai', 'khuda', 'tune', 'duaein', 'suni', 'dil', 'ki', 'sadayein', 'suni', 'tujh', 'se', 'main', 'maangu', 'aur', 'kya', 'tujh', 'bin', 'adhura', 'hoon', 'main', 'tere', 'sang', 'poora', 'hoon', 'main', 'karta', 'hoon', 'tera', 'shukriya', 'kaise', 'kahoon', 'kaise', 'kahoon', 'kaise', 'kahoon', 'kaise', 'kahoon', 'kaise', 'kahoon', 'lamhe', 'mujhe', 'chhoo', 'rahe', 'hain', 'aisa', 'laga', 'hai', 'in', 'mein', 'tera', 'hi', 'to', 'ehsaas', 'hai', 'kaise', 'kahon', 'dil', 'mein', 'nayi', 'aahtein', 'hai', 'aisa', 'laga', 'hai', 'in', 'mein', 'tera', 'hi', 'to', 'ehsaas', 'hai', 'tum', 'mile', 'to', 'mera', 'dil', 'gaya', 'tum', 'mile', 'to', 'sab', 'kuch', 'mil', 'gaya', 'tum', 'mile', 'to', 'logon', 'se', 'kya', 'vaasta', 'tum', 'mile', 'to', 'jadoo', 'chhaa', 'gaya', 'tum', 'mile', 'to', 'jeena', 'aa', 'gaya', 'tum', 'mile', 'to', 'maine', 'paya', 'hai', 'khuda']\n",
      "Zara sa.txt: ['Zara', 'Si', 'dil', 'mein', 'de', 'jagah', 'tuZara', 'sa', 'apna', 'le', 'banaZara', 'sa', 'khwabon', 'mein', 'saja', 'tuZara', 'sa', 'yaadon', 'mein', 'basaMain', 'chahun', 'tujhkoMeri', 'jaan', 'bepanaahFida', 'hoon', 'tujhpeMeri', 'jaan', 'bepanaahZara', 'Si', 'dil', 'mein', 'de', 'jagah', 'tuZara', 'sa', 'apna', 'le', 'banaZara', 'sa', 'khwabon', 'mein', 'saja', 'tuZara', 'sa', 'yaadon', 'mein', 'basaMain', 'tere', 'main', 'tereKadmon', 'mein', 'rakh', 'du', 'yeh', 'jahaanMera', 'ishq', 'deewangiHai', 'nahi', 'hai', 'nahiAashiq', 'koi', 'mujhsa', 'teraTu', 'mere', 'liye', 'bandagiMain', 'chahun', 'tujhkoMeri', 'jaan', 'bepanaahFida', 'hoon', 'tujhpeMeri', 'jaan', 'bepanaahZara', 'Si', 'dil', 'mein', 'de', 'jagah', 'tuZara', 'sa', 'apna', 'le', 'banaZara', 'sa', 'khwabon', 'mein', 'saja', 'tuZara', 'sa', 'yaadon', 'mein', 'basaKeh', 'bhi', 'di', 'keh', 'bhi', 'deDil', 'mein', 'tere', 'jo', 'hai', 'chupaKhwahish', 'jo', 'hai', 'teriRakh', 'nahi', 'rakh', 'nahiParda', 'koi', 'mujhse', 'aye', 'jaanKar', 'le', 'tu', 'mera', 'yakinMein', 'chahun', 'tujhkoMeri', 'jaan', 'bepanaahFida', 'hoon', 'tujhpeMeri', 'jaan', 'bepanaah']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load text data from JSON file\n",
    "def load_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Tokenize text into words\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Path to the JSON file containing text data\n",
    "json_file_path = './lyrics/output.json'\n",
    "\n",
    "# Load data from JSON file\n",
    "data = load_data_from_json(json_file_path)\n",
    "\n",
    "# Tokenize each text entry in the JSON file\n",
    "tokenized_data = {}\n",
    "for key, value in data.items():\n",
    "    tokenized_data[key] = tokenize_text(value)\n",
    "\n",
    "# Print tokenized data\n",
    "for key, tokens in tokenized_data.items():\n",
    "    print(f\"{key}: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training LSTM Networks with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "\n",
    "# Load text data from JSON file\n",
    "def load_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_data(data):\n",
    "    texts = list(data.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Convert text to sequences of tokens\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    max_sequence_len = max([len(seq) for seq in sequences])\n",
    "\n",
    "    # Pad sequences to ensure uniform length\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    return sequences, total_words, max_sequence_len, tokenizer\n",
    "\n",
    "# Build LSTM model with regularization\n",
    "def build_model(total_words, max_sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words / 2, activation='relu'))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Load data from JSON file\n",
    "json_file_path = './lyrics/output.json'\n",
    "data = load_data_from_json(json_file_path)\n",
    "\n",
    "# Preprocess data\n",
    "sequences, total_words, max_sequence_len, tokenizer = preprocess_data(data)\n",
    "\n",
    "# Prepare input and output sequences\n",
    "input_sequences, labels = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "# Build LSTM model\n",
    "model = build_model(total_words, max_sequence_len)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_sequences, labels, epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('lyrics_generator_model.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
